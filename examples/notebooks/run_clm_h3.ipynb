{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1ucky40nc3/feed_h3/blob/main/examples/notebooks/run_clm_h3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KUmrCVpceJr"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMnwOzUk-UoB"
      },
      "outputs": [],
      "source": [
        "# @title Install Dependencies\n",
        "\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "!pip install einops\n",
        "!pip install pytorch-lightning\n",
        "!pip install flash-attn\n",
        "!pip install pykeops\n",
        "!pip install flatten-dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBce_qgv_hw7"
      },
      "source": [
        "# Install [H3](https://github.com/HazyResearch/H3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKaGO1I1-x_D"
      },
      "outputs": [],
      "source": [
        "!git clone --recursive https://github.com/HazyResearch/H3.git\n",
        "%cd H3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNvN6DuA-p2D"
      },
      "outputs": [],
      "source": [
        "# @title Import Dependencies\n",
        "\n",
        "from typing import (\n",
        "    Optional,\n",
        "    Tuple,\n",
        "    Union,\n",
        "    Dict,\n",
        "    List,\n",
        "    Any\n",
        ")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import copy\n",
        "import json\n",
        "import random\n",
        "from dataclasses import (\n",
        "    field,\n",
        "    dataclass\n",
        ")\n",
        "from itertools import chain\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import datasets\n",
        "from datasets import (\n",
        "    load_dataset,\n",
        "    DatasetDict,\n",
        "    get_dataset_infos,\n",
        "    get_dataset_config_names\n",
        ")\n",
        "\n",
        "import evaluate\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoTokenizer,\n",
        "    default_data_collator,\n",
        "    get_scheduler\n",
        ")\n",
        "from transformers.utils import ModelOutput\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import set_seed\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import flatten_dict\n",
        "\n",
        "from flash_attn.utils.generation import InferenceParams\n",
        "\n",
        "from src.models.ssm_seq import SSMLMHeadModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNXpaqZqBaCl"
      },
      "source": [
        "# Prepare the Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PYLbb_b_yvU"
      },
      "outputs": [],
      "source": [
        "# @title Implement Configs\n",
        "\n",
        "@dataclass\n",
        "class Dataclass:\n",
        "    def keys(self):\n",
        "        return self.__dict__.keys()\n",
        "\n",
        "    def __setitem__(self, item: Any, key: str) -> None:\n",
        "        setattr(self, key, item)\n",
        "\n",
        "    def __getitem__(self, key: str) -> Any:\n",
        "        return getattr(self, key)\n",
        "\n",
        "@dataclass\n",
        "class SSMConfig(Dataclass):\n",
        "    head_dim: int = 1\n",
        "    d_state: int = 64\n",
        "    dropout: float = 0.0\n",
        "    mode: str = 'diag'\n",
        "    measure: str = 'diag-lin'\n",
        "    use_fast_fftconv: bool = False\n",
        "\n",
        "@dataclass\n",
        "class AttnConfig(Dataclass):\n",
        "    num_heads: int = 12\n",
        "    bias: bool = True\n",
        "    dropout: float = 0.0\n",
        "    rotary_emb_dim: Optional[int] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.rotary_emb_dim in [None, 64], \\\n",
        "            'The `rotary_emb_dim` can either be `None`/`64`.'\n",
        "        \n",
        "        if self.rotary_emb_dim is None:\n",
        "            self.rotary_emb_dim = 0\n",
        "\n",
        "@dataclass\n",
        "class SSMModelConfig(Dataclass):\n",
        "    d_model: int = 768\n",
        "    n_layer: int = 12\n",
        "    ssm_cfg: SSMConfig = SSMConfig()\n",
        "    attn_cfg: AttnConfig = AttnConfig()\n",
        "    resid_dropout: float = 0.0\n",
        "    embed_dropout: float = 0.1\n",
        "    layer_norm_epsilon: float = 1e-5\n",
        "    d_inner: Optional[int] = None\n",
        "    attn_layer_idx: Optional[List[int]] = field(\n",
        "        default_factory=lambda: [6]\n",
        "    )\n",
        "    fused_mlp: bool = False\n",
        "    fused_dropout_add_ln: bool = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.d_inner is None:\n",
        "            self.d_inner = 4 * self.d_model\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments(Dataclass):\n",
        "    model_name_or_path: Optional[str] = None\n",
        "    config_name: Optional[str] = None\n",
        "    tokenizer_name: Optional[str] = None\n",
        "    use_fast_tokenizer: bool = True\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments(Dataclass):\n",
        "    dataset_name: Optional[str] = None\n",
        "    dataset_config_name: Optional[str] = None\n",
        "    train_file: Optional[str] = None\n",
        "    validation_split_percentage: Optional[int] = 5\n",
        "    max_seq_length: Optional[int] = None\n",
        "    pad_to_max_length: bool = False\n",
        "    max_train_samples: Optional[int] = None\n",
        "    max_eval_samples: Optional[int] = None\n",
        "    preprocessing_num_workers: Optional[int] = None\n",
        "    block_size: Optional[int] = None\n",
        "    keep_linebreaks: bool = True\n",
        "    overwrite_cache: bool = False\n",
        "\n",
        "    def __post__init__(self):\n",
        "        def ext(path):\n",
        "            f = os.path.split(path)[-1]\n",
        "            _, ext = os.path.splitext(f)[-1]\n",
        "            return ext\n",
        "\n",
        "        if self.train_file is not None:\n",
        "            assert ext(self.train_file) in ['csv', 'json', 'txt']\n",
        "        if self.validation_file is not None:\n",
        "            assert ext(self.validation_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtmIyb3yBcve"
      },
      "outputs": [],
      "source": [
        "# @title Implement Utils\n",
        "\n",
        "def now(format='%Y%m%d%H%M%S'):\n",
        "    return datetime.now().strftime(format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms4bNQ-BBd7J"
      },
      "outputs": [],
      "source": [
        "# @title Initialize the Config\n",
        "\n",
        "# SSMModel params\n",
        "D_MODEL = 1024\n",
        "D_INNER = 4 * D_MODEL\n",
        "N_LAYER = 12\n",
        "RESID_DROPOUT = 0.0\n",
        "EMBED_DROPOUT = 0.1\n",
        "LAYER_NORM_EPSILON = 1e-5\n",
        "FUSED_MLP = False\n",
        "FUSED_DROPUT_ADD_LN = False\n",
        "# SSM params\n",
        "SSM_CFG_HEAD_DIM = 8\n",
        "SSM_CFG_D_STATE = 64\n",
        "SSM_CFG_DROPUT = 0.0\n",
        "SSM_CFG_USE_FAST_FFTCONV = False\n",
        "# MHA params\n",
        "ATTN_CFG_NUM_HEADS = 12\n",
        "ATTN_CFG_BIAS = True\n",
        "ATTN_CFG_DROPOUT = 0.1\n",
        "ATTN_CFG_ROTARY_EMB_DIM = None\n",
        "\n",
        "# Select SSMModel params config\n",
        "CUSTOM_SSM_MODEL_CONFIG = False\n",
        "SSM_MODEL_CONFIGS = ['125M', '125M_hybrid']\n",
        "SSM_MODEL_CONFIG = '125M'\n",
        "assert SSM_MODEL_CONFIG in SSM_MODEL_CONFIGS, f'The `SSM_CONFIG` is not in {SSM_MODEL_CONFIGS}!'\n",
        "\n",
        "# Model args\n",
        "MODEL_NAME_OR_PATH = 'google/byt5-small'\n",
        "CONFIG_NAME = None\n",
        "TOKENIZER_NAME = None\n",
        "\n",
        "# Dataset args\n",
        "DATASET_NAME = 'tiny_shakespeare'\n",
        "TRAIN_FILE = None\n",
        "VALIDATION_SPLIT_PERCENTAGE = 5\n",
        "BLOCK_SIZE = 128\n",
        "PAD_TO_MAX_LENGTH = False\n",
        "MAX_TRAIN_SAMPLES = None\n",
        "MAX_EVAL_SAMPLES = None\n",
        "PREPROCESSING_NUM_WORKERS = None\n",
        "\n",
        "# Training args\n",
        "OUTPUT_DIR = f'./runs/{now()}'\n",
        "DO_TRAIN = True\n",
        "DO_EVAL = True\n",
        "NUM_TRAIN_EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "LEARNING_RATE = 5e-5\n",
        "FP16 = True\n",
        "LOGGING_DIR = f'./runs/logs'\n",
        "REPORT_TO = 'tensorboard'\n",
        "SEED = 42\n",
        "SAVE_TOTAL_LIMIT = 3\n",
        "\n",
        "\n",
        "if CUSTOM_SSM_MODEL_CONFIG:\n",
        "    model_config = SSMModelConfig(\n",
        "        d_model=D_MODEL,\n",
        "        d_inner=D_INNER,\n",
        "        n_layer=N_LAYER,\n",
        "        ssm_cfg=SSMConfig(\n",
        "            head_dim=SSM_CFG_HEAD_DIM,\n",
        "            d_state=SSM_CFG_D_STATE,\n",
        "            dropout=SSM_CFG_DROPUT,\n",
        "            use_fast_fftconv=SSM_CFG_USE_FAST_FFTCONV\n",
        "        ),\n",
        "        attn_cfg=AttnConfig(\n",
        "            num_heads=ATTN_CFG_NUM_HEADS,\n",
        "            bias=ATTN_CFG_BIAS,\n",
        "            dropout=ATTN_CFG_DROPOUT,\n",
        "            rotary_emb_dim=ATTN_CFG_ROTARY_EMB_DIM\n",
        "        ),\n",
        "        resid_dropout=RESID_DROPOUT,\n",
        "        embed_dropout=EMBED_DROPOUT,\n",
        "        layer_norm_epsilon=LAYER_NORM_EPSILON,\n",
        "        fused_mlp=FUSED_MLP,\n",
        "        fused_dropout_add_ln=FUSED_DROPUT_ADD_LN\n",
        "    )\n",
        "elif SSM_MODEL_CONFIG == '125M':\n",
        "    model_config = SSMModelConfig(\n",
        "        d_model=768,\n",
        "        n_layer=12,\n",
        "        ssm_cfg=SSMConfig(\n",
        "            head_dim=8,\n",
        "        ),\n",
        "        attn_layer_idx=None,\n",
        "        attn_cfg=AttnConfig(\n",
        "            num_heads=12,\n",
        "            rotary_emb_dim=None\n",
        "        ),\n",
        "    )\n",
        "elif SSM_MODEL_CONFIG == '125M_hybrid':\n",
        "    model_config = SSMModelConfig()\n",
        "else:\n",
        "    raise ValueError(f'The `SSM_CONFIG` is not in {SSM_MODEL_CONFIGS}!')\n",
        "\n",
        "model_args = ModelArguments(\n",
        "    model_name_or_path=MODEL_NAME_OR_PATH,\n",
        "    config_name=CONFIG_NAME,\n",
        "    tokenizer_name=TOKENIZER_NAME\n",
        ")\n",
        "data_args = DataTrainingArguments(\n",
        "    dataset_name=DATASET_NAME,\n",
        "    train_file=TRAIN_FILE,\n",
        "    validation_split_percentage=VALIDATION_SPLIT_PERCENTAGE,\n",
        "    block_size=BLOCK_SIZE,\n",
        "    pad_to_max_length=PAD_TO_MAX_LENGTH,\n",
        "    max_train_samples=MAX_TRAIN_SAMPLES,\n",
        "    max_eval_samples=MAX_EVAL_SAMPLES,\n",
        "    preprocessing_num_workers=PREPROCESSING_NUM_WORKERS\n",
        ")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    do_train=DO_TRAIN,\n",
        "    do_eval=DO_EVAL,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    fp16=FP16,\n",
        "    logging_dir=LOGGING_DIR,\n",
        "    report_to=REPORT_TO,\n",
        "    seed=SEED,\n",
        "    data_seed=SEED,\n",
        "    save_total_limit=SAVE_TOTAL_LIMIT\n",
        ")\n",
        "set_seed(training_args.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlnQNKraDYFp"
      },
      "source": [
        "# Initialize the Accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx5-lufzCWr-"
      },
      "outputs": [],
      "source": [
        "accelerator_kwargs = {\n",
        "    'mixed_precision': None if training_args.fp16 is False else 'fp16',\n",
        "    'gradient_accumulation_steps': training_args.gradient_accumulation_steps,\n",
        "    'log_with': training_args.report_to,\n",
        "    'project_dir': training_args.logging_dir,\n",
        "}\n",
        "accelerator = Accelerator(**accelerator_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-nvcXufDbFO"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZpHfcTjDdDF"
      },
      "outputs": [],
      "source": [
        "if data_args.dataset_name is not None:\n",
        "    dataset_config_name = data_args.dataset_config_name\n",
        "    if dataset_config_name is None:\n",
        "        dataset_config_names = get_dataset_config_names(data_args.dataset_name)\n",
        "        assert len(dataset_config_names) == 1, f'The dataset has multiple `configs`! Choose one of: {dataset_config_names}'\n",
        "        dataset_config_name = dataset_config_names[0]\n",
        "    infos = get_dataset_infos(data_args.dataset_name)[dataset_config_name]\n",
        "\n",
        "    if 'validation' not in infos.splits.keys():\n",
        "        raw_datasets = DatasetDict()\n",
        "        raw_datasets['validation'] = load_dataset(\n",
        "            data_args.dataset_name,\n",
        "            data_args.dataset_config_name,\n",
        "            split=f'train[:{data_args.validation_split_percentage}%]',\n",
        "        )\n",
        "        raw_datasets['train'] = load_dataset(\n",
        "            data_args.dataset_name,\n",
        "            data_args.dataset_config_name,\n",
        "            split=f'train[{data_args.validation_split_percentage}%:]',\n",
        "        )\n",
        "    else:\n",
        "        raw_datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\n",
        "\n",
        "else:\n",
        "    data_files = {}\n",
        "    dataset_args = {}\n",
        "    if data_args.train_file is not None:\n",
        "        data_files['train'] = data_args.train_file\n",
        "    if data_args.validation_file is not None:\n",
        "        data_files['validation'] = data_args.validation_file\n",
        "\n",
        "    extension = data_args.train_file.split('.')[-1]\n",
        "    if extension == 'txt':\n",
        "        extension = 'text'\n",
        "        dataset_args['keep_linebreaks'] = not data_args.no_keep_linebreaks\n",
        "    \n",
        "    if 'validation' not in data_files.keys():\n",
        "        raw_datasets = DatasetDict()\n",
        "        raw_datasets['validation'] = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            split=f'train[:{data_args.validation_split_percentage}%]',\n",
        "            **dataset_args,\n",
        "        )\n",
        "        raw_datasets['train'] = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            split=f'train[{data_args.validation_split_percentage}%:]',\n",
        "            **dataset_args,\n",
        "        )\n",
        "    else:\n",
        "        raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKOcFBEsGlIX"
      },
      "source": [
        "# Initialize the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIIlIy4iGhHX"
      },
      "outputs": [],
      "source": [
        "if model_args.tokenizer_name:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name, \n",
        "        use_fast=model_args.use_fast_tokenizer\n",
        "    )\n",
        "elif model_args.model_name_or_path:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path, \n",
        "        use_fast=model_args.use_fast_tokenizer\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(\n",
        "        'You are instantiating a new tokenizer from scratch. This is not supported by this script.'\n",
        "        'You can do it from another script, save it, and load it from here, using --tokenizer_name.'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPNDWhE7Gzz9"
      },
      "source": [
        "# Preprocess the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRO_5E0fGrz4"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the datasets.\n",
        "# First we tokenize all the texts.\n",
        "column_names = raw_datasets['train'].column_names\n",
        "text_column_name = 'text' if 'text' in column_names else column_names[0]\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[text_column_name])\n",
        "\n",
        "with accelerator.main_process_first():\n",
        "    tokenized_datasets = raw_datasets.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        num_proc=data_args.preprocessing_num_workers,\n",
        "        remove_columns=column_names,\n",
        "        load_from_cache_file=not data_args.overwrite_cache,\n",
        "        desc='Running tokenizer on dataset',\n",
        "    )\n",
        "\n",
        "if data_args.block_size is None:\n",
        "    block_size = tokenizer.model_max_length\n",
        "    if block_size > 1024:\n",
        "        print(\n",
        "            'The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value'\n",
        "            ' of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can'\n",
        "            ' override this default with `--block_size xxx`.'\n",
        "        )\n",
        "    block_size = 1024\n",
        "else:\n",
        "    if data_args.block_size > tokenizer.model_max_length:\n",
        "        print(\n",
        "            f'The block_size passed ({data_args.block_size}) is larger than the maximum length for the model'\n",
        "            f'({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.'\n",
        "        )\n",
        "    block_size = min(data_args.block_size, tokenizer.model_max_length)\n",
        "\n",
        "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result['labels'] = result['input_ids'].copy()\n",
        "    return result\n",
        "\n",
        "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
        "# for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
        "# to preprocess.\n",
        "#\n",
        "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
        "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
        "\n",
        "with accelerator.main_process_first():\n",
        "    lm_datasets = tokenized_datasets.map(\n",
        "        group_texts,\n",
        "        batched=True,\n",
        "        num_proc=data_args.preprocessing_num_workers,\n",
        "        load_from_cache_file=not data_args.overwrite_cache,\n",
        "        desc=f'Grouping texts in chunks of {block_size}',\n",
        "    )\n",
        "\n",
        "train_dataset = lm_datasets['train']\n",
        "eval_dataset = lm_datasets['validation']\n",
        "\n",
        "# Log a few random samples from the training set:\n",
        "for index in random.sample(range(len(train_dataset)), 3):\n",
        "    print(f'Sample {index} of the training set: {train_dataset[index]}.')\n",
        "\n",
        "# DataLoaders creation:\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, \n",
        "    shuffle=True, \n",
        "    collate_fn=default_data_collator, \n",
        "    batch_size=training_args.per_device_train_batch_size\n",
        ")\n",
        "eval_dataloader = torch.utils.data.DataLoader(\n",
        "    eval_dataset, \n",
        "    collate_fn=default_data_collator, \n",
        "    batch_size=training_args.per_device_eval_batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAuhmU3ILJN"
      },
      "source": [
        "# Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Khhp4Tx5cV9"
      },
      "outputs": [],
      "source": [
        "class CausalLMOutput(ModelOutput):\n",
        "    logits: torch.FloatTensor = None\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "\n",
        "\n",
        "class SSMModelForCausalLM(nn.Module):\n",
        "    def __init__(self, config: SSMModelConfig, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.model = SSMLMHeadModel(\n",
        "            model_config.d_model, \n",
        "            n_layer=model_config.n_layer, \n",
        "            d_inner=4 * model_config.d_model, \n",
        "            vocab_size=len(tokenizer),\n",
        "            ssm_cfg=model_config.ssm_cfg, \n",
        "            attn_layer_idx=model_config.attn_layer_idx, \n",
        "            attn_cfg=model_config.attn_cfg,\n",
        "            pad_vocab_size_multiple=8\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        inference_params: Optional[InferenceParams] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        **kwargs\n",
        "    ) -> CausalLMOutput:\n",
        "        logits = self.model(\n",
        "            input_ids,\n",
        "            position_ids,\n",
        "            inference_params\n",
        "        ).logits\n",
        "\n",
        "        loss = None \n",
        "        if labels is not None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(\n",
        "                shift_logits.view(-1, shift_logits.size(-1)), \n",
        "                shift_labels.view(-1)\n",
        "            )\n",
        "        \n",
        "        return CausalLMOutput(\n",
        "            loss=loss,\n",
        "            logits=logits\n",
        "        )\n",
        "\n",
        "    def generate(self, *args, **kwargs) -> Any:\n",
        "        return self.model.generate(*args, **kwargs)\n",
        "\n",
        "\n",
        "model = SSMModelForCausalLM(model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on-yLNKpIDAK"
      },
      "source": [
        "# Initialize the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG6_Qh5LIFRL"
      },
      "outputs": [],
      "source": [
        "no_decay = ['bias', 'layer_norm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        'params': [\n",
        "            p \n",
        "            for n, p in model.named_parameters() \n",
        "            if not any(nd in n for nd in no_decay)\n",
        "        ],\n",
        "        'weight_decay': training_args.weight_decay,\n",
        "    },\n",
        "    {\n",
        "        'params': [\n",
        "            p \n",
        "            for n, p in model.named_parameters() \n",
        "            if any(nd in n for nd in no_decay)\n",
        "        ],\n",
        "        'weight_decay': 0.0,\n",
        "    },\n",
        "]\n",
        "optimizer = torch.optim.AdamW(\n",
        "    optimizer_grouped_parameters, \n",
        "    lr=training_args.learning_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSw9USgxNNI4"
      },
      "source": [
        "# Initialize the Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-vwf1ahNPg2"
      },
      "outputs": [],
      "source": [
        "overwrote_max_train_steps = False\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / training_args.gradient_accumulation_steps)\n",
        "if training_args.max_steps in (None, -1):\n",
        "    training_args.max_steps = training_args.num_train_epochs * num_update_steps_per_epoch\n",
        "    overwrote_max_train_steps = True\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=training_args.lr_scheduler_type,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=training_args.warmup_steps * training_args.gradient_accumulation_steps,\n",
        "    num_training_steps=training_args.max_steps * training_args.gradient_accumulation_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oma2HVhNhg8"
      },
      "source": [
        "# Put Everything on the Accelerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ6sj9k7NfpK"
      },
      "outputs": [],
      "source": [
        "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwibh51YN9XB"
      },
      "source": [
        "# Prepare Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T-bzXGZOGSU"
      },
      "outputs": [],
      "source": [
        "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / training_args.gradient_accumulation_steps)\n",
        "if overwrote_max_train_steps:\n",
        "    training_args.max_steps = int(training_args.num_train_epochs * num_update_steps_per_epoch)\n",
        "# Afterwards we recalculate our number of training epochs\n",
        "training_args.num_train_epochs = math.ceil(training_args.max_steps / num_update_steps_per_epoch)\n",
        "\n",
        "# Figure out how many steps we should save the Accelerator states\n",
        "checkpointing_steps = training_args.save_steps\n",
        "if checkpointing_steps is not None and isinstance(checkpointing_steps, str) and checkpointing_steps.isdigit():\n",
        "    checkpointing_steps = int(checkpointing_steps)\n",
        "\n",
        "total_batch_size = training_args.per_device_train_batch_size * accelerator.num_processes * training_args.gradient_accumulation_steps\n",
        "\n",
        "experiment_config = {**vars(training_args), **vars(data_args), **vars(model_args), **vars(model_config)}\n",
        "# TensorBoard cannot log Enums, need the raw value\n",
        "experiment_config['lr_scheduler_type'] = experiment_config['lr_scheduler_type'].value\n",
        "experiment_config = {k: v for k, v in experiment_config.items() if not k.startswith('_')}\n",
        "experiment_config = flatten_dict.flatten(experiment_config, reducer='path')\n",
        "cast = lambda a: a if isinstance(a, (int, float, str, bool, torch.Tensor)) else str(a)\n",
        "experiment_config = {k: cast(v) for k, v in experiment_config.items() if not k.startswith('_')}\n",
        "accelerator.init_trackers('clm_no_trainer', experiment_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNGOAr2MORNL"
      },
      "source": [
        "# Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4btyf_hbKRf"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir $LOGGING_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgBvCjvlOSZg"
      },
      "outputs": [],
      "source": [
        "print('***** Running training *****')\n",
        "print(f'  Num examples = {len(train_dataset)}')\n",
        "print(f'  Num Epochs = {training_args.num_train_epochs}')\n",
        "print(f'  Instantaneous batch size per device = {training_args.per_device_train_batch_size}')\n",
        "print(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')\n",
        "print(f'  Gradient Accumulation steps = {training_args.gradient_accumulation_steps}')\n",
        "print(f'  Total optimization steps = {training_args.max_steps}')\n",
        "# Only show the progress bar once on each machine.\n",
        "progress_bar = tqdm(range(training_args.max_steps), disable=not accelerator.is_local_main_process)\n",
        "completed_steps = 0\n",
        "starting_epoch = 0\n",
        "\n",
        "for epoch in range(starting_epoch, training_args.num_train_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        with accelerator.accumulate(model):\n",
        "            output = model(**batch)\n",
        "            loss = output.loss\n",
        "            total_loss += loss.detach().float()\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        if accelerator.sync_gradients:\n",
        "            progress_bar.update(1)\n",
        "            completed_steps += 1\n",
        "        \n",
        "        if isinstance(checkpointing_steps, int):\n",
        "            if completed_steps % checkpointing_steps == 0:\n",
        "                output_dir = f'step_{completed_steps }'\n",
        "                if training_args.output_dir is not None:\n",
        "                    output_dir = os.path.join(training_args.output_dir, output_dir)\n",
        "                accelerator.save_state(output_dir)\n",
        "        if completed_steps >= training_args.max_steps:\n",
        "            break\n",
        "\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            output = model(**batch)\n",
        "        loss = output.loss\n",
        "        losses.append(\n",
        "            accelerator.gather_for_metrics(\n",
        "                loss.repeat(training_args.per_device_eval_batch_size)\n",
        "            )\n",
        "        )\n",
        "    losses = torch.cat(losses)\n",
        "    try:\n",
        "        eval_loss = torch.mean(losses)\n",
        "        perplexity = math.exp(eval_loss)\n",
        "    except OverflowError:\n",
        "        perplexity = float('inf')\n",
        "\n",
        "    print(f'epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}')\n",
        "\n",
        "    accelerator.log(\n",
        "        {\n",
        "            'perplexity': perplexity,\n",
        "            'eval_loss': eval_loss,\n",
        "            'train_loss': total_loss.item() / len(train_dataloader),\n",
        "            'epoch': epoch,\n",
        "            'step': completed_steps,\n",
        "        },\n",
        "        step=completed_steps,\n",
        "    )\n",
        "\n",
        "if training_args.output_dir is not None:\n",
        "    output_dir = f'step_{completed_steps}'\n",
        "    if training_args.output_dir is not None:\n",
        "        output_dir = os.path.join(training_args.output_dir, output_dir)\n",
        "    accelerator.save_state(output_dir)\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(training_args.output_dir)\n",
        "\n",
        "        with open(os.path.join(training_args.output_dir, 'all_results.json'), 'w') as f:\n",
        "            json.dump({'perplexity': perplexity}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kj7fzDcO0YR"
      },
      "source": [
        "# Do Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIkwSmbMQXEA"
      },
      "outputs": [],
      "source": [
        "prompt = '\\n'\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "input_ids = inputs.input_ids.to(accelerator.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids=input_ids, \n",
        "        max_length=128,\n",
        "        return_dict_in_generate=False, \n",
        "        output_scores=False, \n",
        "        timing=False, \n",
        "        top_p=0.9, \n",
        "        top_k=50, \n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(tokenizer.batch_decode(output_ids)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ncc3OJ2pDTE"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAA1N7zOpLk5"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPNOK8azpa-U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNkd4sVU+qICUjjHFSFGbRa",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "66ded959bbbeb6d30108d4392b075a689d35f86d96aa50aa1704186c52e13d9c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
